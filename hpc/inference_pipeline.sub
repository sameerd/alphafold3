
# setting the universe to vanilla will require --container argument
#universe = vanilla
# comment out the line above and uncomment the line below to run the scripts
# inside the container. Also, remove the --container argument
# from arguments
container_image = file:///staging/groups/glbrc_alphafold/af3/alphafold3.minimal.22Jan2025.sif

executable = inference_pipeline.sh

log = ../logs/inference_pipeline.log
output = ../logs/inference_pipeline_$(Cluster).out
error = ../logs/inference_pipeline_$(Cluster).err

initialdir = $(directory)
# transfer all files in the inference_inputs directory
transfer_input_files = inference_inputs/

should_transfer_files = YES
when_to_transfer_output = ON_EXIT

# We need this to transfer the model weights to the execute node
Requirements = (Target.HasCHTCStaging == true)

request_memory = 16GB
# need space for the container (3GB) as well
request_disk = 5GB
request_cpus = 4
request_gpus = 1

# we should be able to run short jobs on CUDA_CAPABILITY=7.x but need
# other environment variables and options to be set. This is done automatically
# in inference_pipeline.sh
gpus_minimum_capability = 7.0

# need more GPU memory for larger jobs

# short jobs 4-6 hours so it is okay to use is_resumable
+GPUJobLength = "short"
+WantGPULab = true
+is_resumable = true

#arguments = --container alphafold3.sif --model_param_file /staging/dcosta2/af3/weights/af3.bin --work_dir_ext $(Cluster)_$(Process)
arguments = --model_param_file /staging/dcosta2/af3/weights/af3.bin --work_dir_ext $(Cluster)_$(Process)

queue directory matching job*

