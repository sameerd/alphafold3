
# setting the universe to vanilla will require --container argument
#universe = vanilla
# comment out the line above and uncomment the line below to run the scripts
# inside the container. Remove the --container argument from arguments
container_image = file:///staging/groups/glbrc_alphafold/af3/alphafold3.minimal.22Jan2025.sif

executable = data_pipeline.sh

log = ../logs/data_pipeline.log
output = ../logs/data_pipeline_$(Cluster).out
error = ../logs/data_pipeline_$(Cluster).err

initialdir = $(directory)
# transfer all files in the data_inputs directory
transfer_input_files = data_inputs/

should_transfer_files = YES
when_to_transfer_output = ON_EXIT

# We need this to transfer the databases to the execute node
Requirements = (Target.HasCHTCStaging == true)

if defined USE_SMALL_DB
  # testing requirements
  request_memory = 8GB
  request_disk = 16GB
  request_cpus = 4
  arguments = --smalldb --work_dir_ext $(Cluster)_$(Process) --verbose
#  arguments = --smalldb --work_dir_ext $(Cluster)_$(Process)
else
  # full requirements
  request_memory = 8GB
  request_disk = 700GB
  request_cpus = 8
  arguments = --work_dir_ext $(Cluster)_$(Proc) 
#  arguments = --work_dir_ext $(Cluster)_$(Process) 
endif

queue directory matching job*

