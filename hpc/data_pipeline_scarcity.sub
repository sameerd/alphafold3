
# setting the universe to vanilla will require --container argument
#universe = vanilla
# comment out the line above and uncomment the line below to run the scripts
# inside the container. Remove the --container argument from arguments
container_image = file:///mnt/bigdata/processed_data/computational_biology/containers/alphafold3/alphafold3.minimal.22Jan2025.sif

executable = data_pipeline.sh

log = ../logs/data_pipeline.log
output = ../logs/data_pipeline_$(Cluster)_$(Process).out
error  = ../logs/data_pipeline_$(Cluster)_$(Process).err

initialdir = $(directory)
# transfer all files in the data_inputs directory
transfer_input_files = data_inputs/

should_transfer_files = YES
when_to_transfer_output = ON_EXIT

Requirements = (has_avx == true)

if defined USE_SMALL_DB
  # testing requirements
  request_memory = 8GB
  request_disk = 16GB
  request_cpus = 4
  arguments = --smalldb --work_dir_ext $(Cluster)_$(Process) --extracted_database_path /mnt/bigdata/processed_data/computational_biology/databases/alphafold3_testing_databases --tmpdir $ENV(HOME)/tmp --verbose
else
  # full requirements
  request_memory = 8GB
  request_disk = 8GB
  request_cpus = 8
  arguments = --work_dir_ext $(Cluster)_$(Process) --extracted_database_path /mnt/bigdata/processed_data/computational_biology/databases/alphafold3_public_databases --tmpdir $ENV(HOME)/tmp --random_sleep_minutes 10
endif

queue directory matching job*


